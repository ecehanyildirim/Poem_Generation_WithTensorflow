{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "poemgenerate.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "WGyKZj3bzf9p",
        "UHjdCjDuSvX_",
        "rNnrKn_lL-IJ",
        "LFjSVAlWzf-N",
        "bbmsf23Bymwe",
        "hgsVvVxnymwf",
        "MJdfPmdqzf-R",
        "r6oUuElIMgVx",
        "-ubPo0_9Prjb",
        "LJL0Q0YPY6Ee",
        "kKkD5M6eoSiN",
        "UlUQzwu6EXam",
        "Y4QwTjAM6A2O"
      ],
      "authorship_tag": "ABX9TyMPwNIu2Nq5aCLDXkyfPIP+",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ecehanyildirim/Poem_Generation_WithTensorflow/blob/main/poemgenerate.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WGyKZj3bzf9p"
      },
      "source": [
        "### Import TensorFlow and other libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "yG_n40gFzf9s"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers.experimental import preprocessing\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "import time"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "TXI2evKr79-X",
        "outputId": "9c3c2ba9-9825-43d5-c529-dad73f9f26e3"
      },
      "source": [
        "text = open(\"siir.txt\",'rb').read().decode(encoding='utf-8')\n",
        "#text = open(\"kahramanlık.txt\",'rb').read().decode(encoding='utf-8')\n",
        "print(f'Length of text: {len(text)} characters')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Length of text: 686540 characters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UHjdCjDuSvX_"
      },
      "source": [
        "### Read the data\n",
        "\n",
        "First, look in the text:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Duhg9NrUymwO",
        "outputId": "f9535d8a-3fa2-48bd-912d-bfc983aafe8f"
      },
      "source": [
        "# Take a look at the first 250 characters in text\n",
        "print(text[:250])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r\n",
            "YAD*\r\n",
            "Güzel günlerim vardı yağmurlarla ıslanan,\r\n",
            "Ve güzel gecelerim masallarla dopdolu.\r\n",
            "Her şey, her şey güzeldi, gözyaşı, dünya, zaman,\r\n",
            "Böğürtlen topladığım ıssız, tozlu köy yolu,\r\n",
            "Güzel günlerim vardı yağmurlarla ıslanan.\r\n",
            "Ufacık korumuzda dola\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "IlCgQBRVymwR",
        "outputId": "1a764bfd-8a49-411d-b2db-6214d3b8efff"
      },
      "source": [
        "# The unique characters in the file\n",
        "vocab = sorted(set(text))\n",
        "print(f'{len(vocab)} unique characters')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "109 unique characters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rNnrKn_lL-IJ"
      },
      "source": [
        "## Process the text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LFjSVAlWzf-N"
      },
      "source": [
        "### Vectorize the text\n",
        "\n",
        "Before training, you need to convert the strings to a numerical representation. \n",
        "\n",
        "The `preprocessing.StringLookup` layer can convert each character into a numeric ID. It just needs the text to be split into tokens first."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "a86OoYtO01go",
        "outputId": "07345c42-61fe-4018-9b9a-2a96a702c862"
      },
      "source": [
        "example_texts = ['abcdefg', 'xyz']\n",
        "\n",
        "chars = tf.strings.unicode_split(example_texts, input_encoding='UTF-8')\n",
        "chars"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[b'a', b'b', b'c', b'd', b'e', b'f', b'g'], [b'x', b'y', b'z']]>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 0
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1s4f1q3iqY8f"
      },
      "source": [
        "Now create the `preprocessing.StringLookup` layer:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "6GMlCe3qzaL9"
      },
      "source": [
        "ids_from_chars = preprocessing.StringLookup(\n",
        "    vocabulary=list(vocab))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZmX_jbgQqfOi"
      },
      "source": [
        "It converts form tokens to character IDs, padding with `0`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "WLv5Q_2TC2pc",
        "outputId": "9115a665-321a-4f61-f474-1f328fb35ba9"
      },
      "source": [
        "ids = ids_from_chars(chars)\n",
        "ids"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[59, 60, 61, 62, 63, 64, 65], [82, 83, 84]]>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 0
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tZfqhkYCymwX"
      },
      "source": [
        "Since the goal of this tutorial is to generate text, it will also be important to invert this representation and recover human-readable strings from it. For this you can use `preprocessing.StringLookup(..., invert=True)`.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uenivzwqsDhp"
      },
      "source": [
        "Note: Here instead of passing the original vocabulary generated with `sorted(set(text))` use the `get_vocabulary()` method of the `preprocessing.StringLookup` layer so that the padding and `[UNK]` tokens are set the same way."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Wd2m3mqkDjRj"
      },
      "source": [
        "chars_from_ids = tf.keras.layers.experimental.preprocessing.StringLookup(\n",
        "    vocabulary=ids_from_chars.get_vocabulary(), invert=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pqTDDxS-s-H8"
      },
      "source": [
        "This layer recovers the characters from the vectors of IDs, and returns them as a `tf.RaggedTensor` of characters:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "c2GCh0ySD44s",
        "outputId": "1638ee3e-ebac-412b-fc8a-932b2ebe23ee"
      },
      "source": [
        "chars = chars_from_ids(ids)\n",
        "chars"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[b'a', b'b', b'c', b'd', b'e', b'f', b'g'], [b'x', b'y', b'z']]>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 0
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-FeW5gqutT3o"
      },
      "source": [
        "You can `tf.strings.reduce_join` to join the characters back into strings. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zxYI-PeltqKP"
      },
      "source": [
        "tf.strings.reduce_join(chars, axis=-1).numpy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w5apvBDn9Ind"
      },
      "source": [
        "def text_from_ids(ids):\n",
        "  return tf.strings.reduce_join(chars_from_ids(ids), axis=-1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bbmsf23Bymwe"
      },
      "source": [
        "### The prediction task"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UopbsKi88tm5"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qmxrYDCTy-eL"
      },
      "source": [
        "ids_dataset = tf.data.Dataset.from_tensor_slices(all_ids)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cjH5v45-yqqH"
      },
      "source": [
        "for ids in ids_dataset.take(10):\n",
        "    print(chars_from_ids(ids).numpy().decode('utf-8'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C-G2oaTxy6km"
      },
      "source": [
        "seq_length = 100\n",
        "examples_per_epoch = len(text)//(seq_length+1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ZSYAcQV8OGP"
      },
      "source": [
        "The `batch` method lets you easily convert these individual characters to sequences of the desired size."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BpdjRO2CzOfZ"
      },
      "source": [
        "sequences = ids_dataset.batch(seq_length+1, drop_remainder=True)\n",
        "\n",
        "for seq in sequences.take(1):\n",
        "  print(chars_from_ids(seq))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5PHW902-4oZt"
      },
      "source": [
        "It's easier to see what this is doing if you join the tokens back into strings:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QO32cMWu4a06"
      },
      "source": [
        "for seq in sequences.take(5):\n",
        "  print(text_from_ids(seq).numpy())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UbLcIPBj_mWZ"
      },
      "source": [
        "For training you'll need a dataset of `(input, label)` pairs. Where `input` and \n",
        "`label` are sequences. At each time step the input is the current character and the label is the next character. \n",
        "\n",
        "Here's a function that takes a sequence as input, duplicates, and shifts it to align the input and label for each timestep:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9NGu-FkO_kYU"
      },
      "source": [
        "def split_input_target(sequence):\n",
        "    input_text = sequence[:-1]\n",
        "    target_text = sequence[1:]\n",
        "    return input_text, target_text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WxbDTJTw5u_P"
      },
      "source": [
        "split_input_target(list(\"Tensorflow\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B9iKPXkw5xwa"
      },
      "source": [
        "dataset = sequences.map(split_input_target)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GNbw-iR0ymwj"
      },
      "source": [
        "for input_example, target_example in dataset.take(1):\n",
        "    print(\"Input :\", text_from_ids(input_example).numpy())\n",
        "    print(\"Target:\", text_from_ids(target_example).numpy())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MJdfPmdqzf-R"
      },
      "source": [
        "### Create training batches\n",
        "\n",
        "You used `tf.data` to split the text into manageable sequences. But before feeding this data into the model, you need to shuffle the data and pack it into batches."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p2pGotuNzf-S"
      },
      "source": [
        "# Batch size\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "# Buffer size to shuffle the dataset\n",
        "# (TF data is designed to work with possibly infinite sequences,\n",
        "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
        "# it maintains a buffer in which it shuffles elements).\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "dataset = (\n",
        "    dataset\n",
        "    .shuffle(BUFFER_SIZE)\n",
        "    .batch(BATCH_SIZE, drop_remainder=True)\n",
        "    .prefetch(tf.data.experimental.AUTOTUNE))\n",
        "\n",
        "dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r6oUuElIMgVx"
      },
      "source": [
        "## Build The Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m8gPwEjRzf-Z"
      },
      "source": [
        "This section defines the model as a `keras.Model` subclass (For details see [Making new Layers and Models via subclassing](https://www.tensorflow.org/guide/keras/custom_layers_and_models)). \n",
        "\n",
        "This model has three layers:\n",
        "\n",
        "* `tf.keras.layers.Embedding`: The input layer. A trainable lookup table that will map each character-ID to a vector with `embedding_dim` dimensions;\n",
        "* `tf.keras.layers.GRU`: A type of RNN with size `units=rnn_units` (You can also use an LSTM layer here.)\n",
        "* `tf.keras.layers.Dense`: The output layer, with `vocab_size` outputs. It outputs one logit for each character in the vocabulary. These are the log-likelihood of each character according to the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zHT8cLh7EAsg"
      },
      "source": [
        "# Length of the vocabulary in chars\n",
        "vocab_size = len(vocab)\n",
        "\n",
        "# The embedding dimension\n",
        "embedding_dim = 256\n",
        "\n",
        "# Number of RNN units\n",
        "rnn_units = 1024"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wj8HQ2w8z4iO"
      },
      "source": [
        "class MyModel(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, rnn_units):\n",
        "    super().__init__(self)\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.gru = tf.keras.layers.GRU(rnn_units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True)\n",
        "    self.dense = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "  def call(self, inputs, states=None, return_state=False, training=False):\n",
        "    x = inputs\n",
        "    x = self.embedding(x, training=training)\n",
        "    if states is None:\n",
        "      states = self.gru.get_initial_state(x)\n",
        "    x, states = self.gru(x, initial_state=states, training=training)\n",
        "    x = self.dense(x, training=training)\n",
        "\n",
        "    if return_state:\n",
        "      return x, states\n",
        "    else:\n",
        "      return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IX58Xj9z47Aw"
      },
      "source": [
        "model = MyModel(\n",
        "    # Be sure the vocabulary size matches the `StringLookup` layers.\n",
        "    vocab_size=len(ids_from_chars.get_vocabulary()),\n",
        "    embedding_dim=embedding_dim,\n",
        "    rnn_units=rnn_units)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RkA5upJIJ7W7"
      },
      "source": [
        "For each character the model looks up the embedding, runs the GRU one timestep with the embedding as input, and applies the dense layer to generate logits predicting the log-likelihood of the next character:\n",
        "\n",
        "![A drawing of the data passing through the model](https://github.com/tensorflow/docs/blob/master/site/en/tutorials/text/images/text_generation_training.png?raw=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gKbfm04amhXk"
      },
      "source": [
        "Note: For training you could use a `keras.Sequential` model here. To  generate text later you'll need to manage the RNN's internal state. It's simpler to include the state input and output options upfront, than it is to rearrange the model architecture later. For more details see the [Keras RNN guide](https://www.tensorflow.org/guide/keras/rnn#rnn_state_reuse)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ubPo0_9Prjb"
      },
      "source": [
        "## Try the model\n",
        "\n",
        "Now run the model to see that it behaves as expected.\n",
        "\n",
        "First check the shape of the output:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C-_70kKAPrPU"
      },
      "source": [
        "for input_example_batch, target_example_batch in dataset.take(1):\n",
        "    example_batch_predictions = model(input_example_batch)\n",
        "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q6NzLBi4VM4o"
      },
      "source": [
        "In the above example the sequence length of the input is `100` but the model can be run on inputs of any length:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vPGmAAXmVLGC"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uwv0gEkURfx1"
      },
      "source": [
        "To get actual predictions from the model you need to sample from the output distribution, to get actual character indices. This distribution is defined by the logits over the character vocabulary.\n",
        "\n",
        "Note: It is important to _sample_ from this distribution as taking the _argmax_ of the distribution can easily get the model stuck in a loop.\n",
        "\n",
        "Try it for the first example in the batch:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4V4MfFg0RQJg"
      },
      "source": [
        "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
        "sampled_indices = tf.squeeze(sampled_indices, axis=-1).numpy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QM1Vbxs_URw5"
      },
      "source": [
        "This gives us, at each timestep, a prediction of the next character index:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YqFMUQc_UFgM"
      },
      "source": [
        "sampled_indices"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LfLtsP3mUhCG"
      },
      "source": [
        "Decode these to see the text predicted by this untrained model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xWcFwPwLSo05"
      },
      "source": [
        "print(\"Input:\\n\", text_from_ids(input_example_batch[0]).numpy())\n",
        "print()\n",
        "print(\"Next Char Predictions:\\n\", text_from_ids(sampled_indices).numpy())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LJL0Q0YPY6Ee"
      },
      "source": [
        "## Train the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YCbHQHiaa4Ic"
      },
      "source": [
        "At this point the problem can be treated as a standard classification problem. Given the previous RNN state, and the input this time step, predict the class of the next character."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "trpqTWyvk0nr"
      },
      "source": [
        "### Attach an optimizer, and a loss function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UAjbjY03eiQ4"
      },
      "source": [
        "The standard `tf.keras.losses.sparse_categorical_crossentropy` loss function works in this case because it is applied across the last dimension of the predictions.\n",
        "\n",
        "Because your model returns logits, you need to set the `from_logits` flag.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZOeWdgxNFDXq"
      },
      "source": [
        "loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4HrXTACTdzY-"
      },
      "source": [
        "example_batch_loss = loss(target_example_batch, example_batch_predictions)\n",
        "mean_loss = example_batch_loss.numpy().mean()\n",
        "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
        "print(\"Mean loss:        \", mean_loss)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vkvUIneTFiow"
      },
      "source": [
        "A newly initialized model shouldn't be too sure of itself, the output logits should all have similar magnitudes. To confirm this you can check that the exponential of the mean loss is approximately equal to the vocabulary size. A much higher loss means the model is sure of its wrong answers, and is badly initialized:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MAJfS5YoFiHf"
      },
      "source": [
        "tf.exp(mean_loss).numpy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jeOXriLcymww"
      },
      "source": [
        "Configure the training procedure using the `tf.keras.Model.compile` method. Use `tf.keras.optimizers.Adam` with default arguments and the loss function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DDl1_Een6rL0"
      },
      "source": [
        "model.compile(optimizer='adam', loss=loss)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ieSJdchZggUj"
      },
      "source": [
        "### Configure checkpoints"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C6XBUUavgF56"
      },
      "source": [
        "Use a `tf.keras.callbacks.ModelCheckpoint` to ensure that checkpoints are saved during training:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W6fWTriUZP-n"
      },
      "source": [
        "# Directory where the checkpoints will be saved\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "# Name of the checkpoint files\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "\n",
        "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix,\n",
        "    save_weights_only=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Ky3F_BhgkTW"
      },
      "source": [
        "### Execute the training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IxdOA-rgyGvs"
      },
      "source": [
        "To keep training time reasonable, use 10 epochs to train the model. In Colab, set the runtime to GPU for faster training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7yGBE2zxMMHs"
      },
      "source": [
        "EPOCHS = 20"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "UK-hmKjYVoll",
        "outputId": "a3db314c-68aa-4467-eb57-a27258962727"
      },
      "source": [
        "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "106/106 [==============================] - 566s 5s/step - loss: 2.9477\n",
            "Epoch 2/20\n",
            "106/106 [==============================] - 560s 5s/step - loss: 2.1646\n",
            "Epoch 3/20\n",
            "106/106 [==============================] - 561s 5s/step - loss: 2.0145\n",
            "Epoch 4/20\n",
            "106/106 [==============================] - 559s 5s/step - loss: 1.8927\n",
            "Epoch 5/20\n",
            "106/106 [==============================] - 560s 5s/step - loss: 1.7842\n",
            "Epoch 6/20\n",
            "106/106 [==============================] - 559s 5s/step - loss: 1.6921\n",
            "Epoch 7/20\n",
            "106/106 [==============================] - 555s 5s/step - loss: 1.6138\n",
            "Epoch 8/20\n",
            "106/106 [==============================] - 564s 5s/step - loss: 1.5431\n",
            "Epoch 9/20\n",
            "106/106 [==============================] - 565s 5s/step - loss: 1.4752\n",
            "Epoch 10/20\n",
            "106/106 [==============================] - 554s 5s/step - loss: 1.4101\n",
            "Epoch 11/20\n",
            "106/106 [==============================] - 555s 5s/step - loss: 1.3413\n",
            "Epoch 12/20\n",
            "106/106 [==============================] - 560s 5s/step - loss: 1.2711\n",
            "Epoch 13/20\n",
            "106/106 [==============================] - 559s 5s/step - loss: 1.1983\n",
            "Epoch 14/20\n",
            "106/106 [==============================] - 563s 5s/step - loss: 1.1212\n",
            "Epoch 15/20\n",
            "106/106 [==============================] - 557s 5s/step - loss: 1.0413\n",
            "Epoch 16/20\n",
            "106/106 [==============================] - 551s 5s/step - loss: 0.9559\n",
            "Epoch 17/20\n",
            "106/106 [==============================] - 561s 5s/step - loss: 0.8704\n",
            "Epoch 18/20\n",
            "106/106 [==============================] - 558s 5s/step - loss: 0.7836\n",
            "Epoch 19/20\n",
            "106/106 [==============================] - 551s 5s/step - loss: 0.7006\n",
            "Epoch 20/20\n",
            "106/106 [==============================] - 551s 5s/step - loss: 0.6189\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kKkD5M6eoSiN"
      },
      "source": [
        "## Generate text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oIdQ8c8NvMzV"
      },
      "source": [
        "The simplest way to generate text with this model is to run it in a loop, and keep track of the model's internal state as you execute it.\n",
        "\n",
        "![To generate text the model's output is fed back to the input](https://github.com/tensorflow/docs/blob/master/site/en/tutorials/text/images/text_generation_sampling.png?raw=1)\n",
        "\n",
        "Each time you call the model you pass in some text and an internal state. The model returns a prediction for the next character and its new state. Pass the prediction and state back in to continue generating text.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DjGz1tDkzf-u"
      },
      "source": [
        "The following makes a single step prediction:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "iSBU1tHmlUSs"
      },
      "source": [
        "class OneStep(tf.keras.Model):\n",
        "  def __init__(self, model, chars_from_ids, ids_from_chars, temperature=1.0):\n",
        "    super().__init__()\n",
        "    self.temperature = temperature\n",
        "    self.model = model\n",
        "    self.chars_from_ids = chars_from_ids\n",
        "    self.ids_from_chars = ids_from_chars\n",
        "\n",
        "    # Create a mask to prevent \"\" or \"[UNK]\" from being generated.\n",
        "    skip_ids = self.ids_from_chars(['', '[UNK]'])[:, None]\n",
        "    sparse_mask = tf.SparseTensor(\n",
        "        # Put a -inf at each bad index.\n",
        "        values=[-float('inf')]*len(skip_ids),\n",
        "        indices=skip_ids,\n",
        "        # Match the shape to the vocabulary\n",
        "        dense_shape=[len(ids_from_chars.get_vocabulary())])\n",
        "    self.prediction_mask = tf.sparse.to_dense(sparse_mask)\n",
        "\n",
        "  @tf.function\n",
        "  def generate_one_step(self, inputs, states=None):\n",
        "    # Convert strings to token IDs.\n",
        "    input_chars = tf.strings.unicode_split(inputs, 'UTF-8')\n",
        "    input_ids = self.ids_from_chars(input_chars).to_tensor()\n",
        "\n",
        "    # Run the model.\n",
        "    # predicted_logits.shape is [batch, char, next_char_logits]\n",
        "    predicted_logits, states = self.model(inputs=input_ids, states=states,\n",
        "                                          return_state=True)\n",
        "    # Only use the last prediction.\n",
        "    predicted_logits = predicted_logits[:, -1, :]\n",
        "    predicted_logits = predicted_logits/self.temperature\n",
        "    # Apply the prediction mask: prevent \"\" or \"[UNK]\" from being generated.\n",
        "    predicted_logits = predicted_logits + self.prediction_mask\n",
        "\n",
        "    # Sample the output logits to generate token IDs.\n",
        "    predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)\n",
        "    predicted_ids = tf.squeeze(predicted_ids, axis=-1)\n",
        "\n",
        "    # Convert from token ids to characters\n",
        "    predicted_chars = self.chars_from_ids(predicted_ids)\n",
        "\n",
        "    # Return the characters and model state.\n",
        "    return predicted_chars, states"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "fqMOuDutnOxK"
      },
      "source": [
        "one_step_model = OneStep(model, chars_from_ids, ids_from_chars)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p9yDoa0G3IgQ"
      },
      "source": [
        "Run it in a loop to generate some text. Looking at the generated text, you'll see the model knows when to capitalize, make paragraphs and imitates a Shakespeare-like writing vocabulary. With the small number of training epochs, it has not yet learned to form coherent sentences."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "ST7PSyk9t1mT",
        "outputId": "77f7d53f-8cec-40c2-d7bb-de2b1ffa58c9"
      },
      "source": [
        "start = time.time()\n",
        "states = None\n",
        "next_char = tf.constant(['ŞAİR:'])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(1000):\n",
        "  next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
        "  result.append(next_char)\n",
        "\n",
        "result = tf.strings.join(result)\n",
        "end = time.time()\n",
        "print(result[0].numpy().decode('utf-8'), '\\n\\n' + '_'*400)\n",
        "print('\\nRun time:', end - start)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ŞAİR:\r\n",
            "*\r\n",
            "İyi almış bir hüzün vermiş sonsuz\r\n",
            "Bir evin aşk değilimi ben yatağım dizlerim kanadı\r\n",
            "evet bir bir herifi vardı\r\n",
            "çünkü her şeyin yapkımı berabersiz kırımları\r\n",
            "kadınlar olur bir başkaldır evet\r\n",
            "ey vücudunun başladığı benim gece nortanın\r\n",
            "temmuz gecesi kollarımı berit\r\n",
            "ormanı taşıdığımız yaşamaya\r\n",
            "ve dünya insana\r\n",
            "ateş için ama yer yoktu biz halimi bilmazsa\r\n",
            "bir bakoşlar İlâhların güftesi\r\n",
            "yalnız çıplak bir akşamdı\r\n",
            "Ve bütün her şey amı verdiği yer\r\n",
            "üç dirim ekmek ve kesin balbuna kaynttır\r\n",
            "şimdi bunu gömelim\r\n",
            "bayram-ben tami, umutsuzluğunda unutulmuş bir bakkalla ve sıcmaz\r\n",
            "başka trenler var yeşil\r\n",
            "Bir bir şey doğar gibi başladığı bir aşktı,\r\n",
            "bu gece durduğunda unutmuştu\r\n",
            "şimdi bir çağlamak farkettim\r\n",
            "hiç biriniz orospusu\r\n",
            "ölüm bitti\r\n",
            "- ölümün tarihesi donukluğu taşlar pırıl pırıl\r\n",
            "yazıp durdu geçti pasayı seslendir\r\n",
            "herkesi tanımıya\r\n",
            "iştola farfara\r\n",
            "tuttuğum efendimi bekle\r\n",
            "sanırım ölüm ölür ulduğu yerde yarana\r\n",
            "kullanıyor garipliği\r\n",
            "eski ve büyük beyaz bir büçük\r\n",
            "bir akşamda duru \n",
            "\n",
            "________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________\n",
            "\n",
            "Run time: 3.490852117538452\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AM2Uma_-yVIq"
      },
      "source": [
        "The easiest thing you can do to improve the results is to train it for longer (try `EPOCHS = 30`).\n",
        "\n",
        "You can also experiment with a different start string, try adding another RNN layer to improve the model's accuracy, or adjust the temperature parameter to generate more or less random predictions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_OfbI4aULmuj"
      },
      "source": [
        "If you want the model to generate text *faster* the easiest thing you can do is batch the text generation. In the example below the model generates 5 outputs in about the same time it took to generate 1 above. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "ZkLu7Y8UCMT7",
        "outputId": "729961d3-2d73-4727-ab96-7cf8c848938a"
      },
      "source": [
        "start = time.time()\n",
        "states = None\n",
        "next_char = tf.constant(['ROMEO:', 'ROMEO:', 'ROMEO:', 'ROMEO:', 'ROMEO:'])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(1000):\n",
        "  next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
        "  result.append(next_char)\n",
        "\n",
        "result = tf.strings.join(result)\n",
        "end = time.time()\n",
        "print(result, '\\n\\n' + '_'*80)\n",
        "print('\\nRun time:', end - start)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(\n",
            "[b\"ROMEO:\\r\\ng\\xc3\\xb6mlek geyik yarlatayd\\xc4\\xb1\\r\\nhi\\xc3\\xa7biri zaman saatlerinde\\r\\nbiraz sahamlar, onunla d\\xc3\\xb6n\\xc3\\xbcyor \\xc3\\xa7ocuk belle\\r\\nNe iyiydi, Ta\\xc5\\x9fnam\\xc4\\xb1k olmal\\xc4\\xb1yd\\xc4\\xb1\\r\\nPosta mavlucurdu hepimiz\\r\\nHER \\xc4\\xb0TTi\\r\\ns\\xc3\\xb6zsin bir beyaz\\xc4\\xb1nda burday ba\\xc5\\x9fk \\xc3\\xbc\\xc3\\xa7 bir\\r\\nrak\\xc4\\xb1'l\\xc4\\xb1kl\\xc4\\xb1 al umutuzlarla\\r\\nyanl\\xc4\\xb1\\xc5\\x9f bir okkamden ge\\xc3\\xa7ti\\xc4\\x9fim\\r\\nbu m\\xc3\\xbctr\\xc3\\xbcler manzetmelere ve b\\xc3\\xb6ylece bir b\\xc3\\xbczb\\xc3\\xbc\\r\\nve belki de macun satan\\r\\nO \\xc5\\x9fark\\xc4\\xb1 mermirim var m\\xc4\\xb1 yedi\\r\\nba\\xc5\\x9ftan ba\\xc5\\x9f\\xc4\\xb1n ve orospunlu\\r\\nsonsuz gemiciyle d\\xc3\\xbcnyan\\xc4\\xb1n\\r\\nbiz kal\\xc4\\xb1r\\xc4\\xb1z sular\\xc4\\xb1, h\\xc3\\xa2l\\xc3\\xa2 ordusu\\r\\nben bir g\\xc3\\xbcn, bir a\\xc4\\x9f\\xc4\\xb1zlar\\xc4\\xb1n\\xc4\\xb1 yamak a\\xc3\\xa7\\xc4\\xb1l\\xc4\\xb1\\xc5\\x9f gibiyim\\r\\npolis zur gibi kayboluyor\\r\\nbir eski a\\xc4\\x9fac\\xc4\\xb1\\r\\nbir bo\\xc5\\x9f kaldi\\r\\nhan\\xc4\\xb1\\xc3\\xa7 taftan ama \\xc5\\x9feyler yapan ve kahveler\\r\\nhayat kald\\xc4\\xb1r\\xc4\\xb1mlarda\\r\\nk\\xc4\\xb1y\\xc4\\xb1ya al\\xc4\\xb1\\xc5\\x9fmad\\xc4\\xb1m aplan\\xc4\\xb1n, bir a\\xc4\\x9fa\\xc3\\xa7?.. -Uykusuzluk, bu d\\xc3\\xbcnyada, ve anlamal\\xc4\\xb1s die\\r\\nyaln\\xc4\\xb1z bir h\\xc3\\xbcz\\xc3\\xbcn veresi s\\xc4\\xb1ras\\xc4\\xb1\\r\\nkapkara vard\\xc4\\xb1\\xc4\\x9f\\xc4\\xb1m\\xc4\\xb1z mavi \\xc3\\xbclkeliden g\\xc3\\xb6zlerimden iki k\\xc4\\xb1\\xc5\\x9f olsa bir\\r\\nkaram\\xc4\\xb1z\\xc4\\xb1 k\\xc4\\xb1r beni, Bil meleksizdir \\xc3\\xa7ile\\xc4\\x9fini \\xc3\\xa7iziyor\\r\\ntut ki varsa yerdenizdi y\\xc3\\xbcz\\xc3\\xbcn\\xc3\\xbcn tembel\\xc3\\xa2m\\xc4\\xb1\\xc5\\x9f\\r\\nda\\xc4\\x9flar\\xc4\\xb1 varl\\xc4\\xb1\\xc4\\x9f\\xc4\\xb1 dalgam ate\\xc5\\x9f mendilime\\r\\n\\xc5\\x9fapkas\\xc4\\xb1n\\xc4\\xb1 \\xc3\\xa7\\xc4\\xb1kar yok hat\\xc4\\xb1rlay\\xc4\\xb1p uzay\\xc4\\xb1c\\xc4\\xb1\\r\\nkimse ne derse desin \\xc3\\xa7i\"\n",
            " b'ROMEO:K\\r\\nYoluna dikildi\\xc4\\x9fi ilk kuruyoruz.\\r\\nKendi derledi\\xc4\\x9fimi anl\\xc4\\xb1yorsunuz;\\r\\nO kokuyu saklam\\xc4\\xb1\\xc5\\x9f. pi\\xc3\\xa7 diye \\xc4\\xb1\\xc5\\x9f\\xc4\\xb1t\\xc4\\xb1lmaz g\\xc3\\xb6rd\\xc3\\xbcm.\\r\\nBen sevinirim kollar\\xc4\\xb1m var Barbar\\r\\nK\\xc4\\xb1v\\xc4\\xb1r\\xc4\\xb1z k\\xc4\\xb1ld\\xc4\\xb1z\\xc4\\xb1yorum \\xc4\\xb1slan\\xc4\\xb1yorum ka\\xc3\\xa7 ki\\xc5\\x9fi\\r\\nBir geyik kontum kald\\xc4\\xb1k\\r\\ncadde var de bah\\xc3\\xa7ede kurulan\\xc4\\xb1r sanki\\r\\n1 JekTELER\\xc4\\xb0\\r\\n1 Jvah yollarda geliyor \\xc3\\xa7o\\xc4\\x9fulluklar\\xc4\\xb1na.\\r\\nXII\\r\\n\\r\\n48\\r\\n\\r\\n\" Neyvar Bir ge\\xc3\\xa7mi\\xc5\\x9f gibi gitti\\xc4\\x9fimi s\\xc3\\xb6ylemek\\r\\nTanr\\xc4\\xb1 ba\\xc4\\x9flayan sakallar\\xc4\\xb1, uyanm\\xc4\\xb1\\xc5\\x9flar d\\xc3\\xb6v\\xc3\\xbclmek birg\\xc3\\xbcn\\r\\nBilmeden as\\xc4\\xb1l ileriyor var, idim ald\\xc4\\xb1.\\r\\nBen Art\\xc4\\xb1k bir g\\xc3\\xbcvercin ay\\xc4\\xb1\\xc4\\xb1n\\xc4\\xb1 parmak birbirine kadar\\r\\n\\xc5\\x9eimdi ben \\xc3\\xb6rt\\xc3\\xbclmez bir geceyi geliyorsa.\\r\\nNe da diye obur apa\\xc4\\x9fa inci\\r\\nYa bir zamban\\xc4\\xb1 korkar\\xc4\\xb1m\\r\\nbir i\\xc3\\xa7imi gibi\\r\\nsevmek g\\xc3\\xbczel g\\xc3\\xb6llerde gezersin\\r\\npek\\xc3\\xa2ni gelmeseydim s\\xc3\\xb6z temsili\\r\\n\\xc4\\xb1\\xc5\\x9f\\xc4\\xb1klar ta\\xc5\\x9f\\xc4\\xb1yan bak\\xc4\\xb1n bir m\\xc3\\xbczik olmas\\xc4\\xb1\\r\\ntut ki nehir verisi gibi terliyen\\r\\n\\xc3\\xa7\\xc4\\xb1k\\xc4\\xb1p ge\\xc3\\xa7mi\\xc5\\x9fsin\\r\\noy farfara dayalmam\\xc4\\xb1z\\r\\nben bir g\\xc3\\xbcn g\\xc3\\xb6zlerimle g\\xc3\\xb6rd\\xc3\\xbcm\\r\\nvatl\\xc4\\xb1 pembe benden ekimden unuttu\\xc4\\x9fu bu t\\xc3\\xbcrk\\xc3\\xbcl\\r\\nHayri\\'nin ba\\xc5\\x9fka bir yerde\\r\\nsimsiyah bir askerdettir\\r\\nsehimli yer alt\\xc4\\xb1nda bir orkada\\r\\nen koca bin \\xc3\\xb6ld\\xc3\\xbcl\\xc3\\xbck cehven\\'in '\n",
            " b'ROMEO:\\r\\nBir \\xc5\\x9filiz bir ormanda bulu\\xc5\\x9fmamazd\\xc4\\xb1\\r\\nOnda vakit di\\xc5\\x9f garibis do\\xc4\\x9fru\\xc5\\x9fum..\\r\\nAkl\\xc4\\xb1m ak\\xc5\\x9fam b\\xc4\\xb1rakacakt\\xc4\\xb1m\\r\\nMekyup \\xc3\\x82damdan bir garip k\\xc4\\xb1ta\\r\\nEl bilir miyiz \\xc3\\xb6yle avunma\"\\r\\nA J P P ZASA/ KAr\\xc4\\xb1\\xc5\\x9f meyal\\xc4\\xb1n\\xc4\\xb1n sonuna kadar\\r\\nBir ba\\xc5\\x9fka be\\xc5\\x9f bir \\xc3\\xb6yle kolu \\xc4\\xb0spanbol\\'da\\r\\nAkl\\xc4\\xb1m ba\\xc5\\x9f\\xc4\\xb1m verdim, ge\\xc3\\xa7mi\\xc5\\x9fte ya\\xc5\\x9fad\\xc4\\xb1m\\r\\nYap\\xc4\\xb1lacaktan k\\xc4\\xb1rlar beter\\r\\nfarade ta\\xc5\\x9f\\xc4\\xb1y\\xc4\\xb1p g\\xc3\\xbc\\xc3\\xa7leri ve avlundan\\r\\nyelyez ver yolcu itmez\\r\\nsak\\xc4\\xb1n ve anlaml\\xc4\\xb1\\r\\nd\\xc3\\xbcnyay\\xc4\\xb1 unutamam\\r\\ne\\xc4\\x9fece bir \\xc5\\x9fiirler\\r\\n\\xc3\\xa7ok umut uyan\\xc4\\xb1\\xc5\\x9f yeli\\xc5\\x9ftiremeyip\\r\\nge\\xc3\\xa7mi\\xc5\\x9fin bu kadar bibere\\r\\nb\\xc3\\xbcy\\xc3\\xbct\\xc3\\xbcn g\\xc3\\xb6\\xc4\\x9fs\\xc3\\xbcnden\\r\\ngen\\xc3\\xa7 i\\xc5\\x9ften geceyi ve ge\\xc3\\xa7mi\\xc5\\x9f bir elma ay\\xc4\\xb1\\r\\ndeniz karalar\\xc4\\xb1 kenti batars\\xc4\\xb1n g\\xc3\\xbczel misal\\r\\ny\\xc3\\xbcr\\xc3\\xbcrsek su ve demek ke dia onu da kendiler kutulardan bir hi\\xc3\\xa7\\r\\nhayat damlardan el basanesini hat\\xc4\\xb1rla\\r\\nsar\\xc4\\xb1\\xc5\\x9f\\xc4\\xb1n g\\xc3\\xb6z\\xc3\\xbc g\\xc3\\xb6rsem\\r\\nbir hali\\xc3\\xa7e gibi bir orospu\\r\\n n hle o on be\\xc5\\x9f ki\\xc5\\x9fiyi\\r\\nekimle ninni\\r\\nGazete parmaklar\\xc4\\xb1nda sahipsiz hayvan\\xc4\\xb1\\r\\ndeniz feberinle girdi\\xc4\\x9fini maki s\\xc3\\xb6yledim\\r\\nk\\xc4\\xb1rm\\xc4\\xb1z\\xc4\\xb1 kiremitten geliyorlar, bir kurtut mistin leyl\\xc3\\xa2klara \\xc3\\xa7\\xc4\\xb1kmak\\r\\nbir \\xc3\\xa7i\\xc3\\xa7ek\\r\\nal\\xc4\\xb1\\xc5\\x9ft\\xc4\\xb1\\xc4\\x9f\\xc4\\xb1 bir yerlerde\\r\\ndaha d\\xc3\\xb6nen kesiyor\\r\\nsenin hi\\xc3\\xa7 bayram dese '\n",
            " b'ROMEO:\\r\\nI\\xc5\\x9f\\xc4\\xb1l\\xc4\\xb1p so\\xc4\\x9fuk kusuldu\\xc4\\x9fum ve demezler. Seni\\r\\n\"K\\xc4\\xb1rm\\xc4\\xb1z\\xc4\\xb1 ve mezarl\\xc4\\xb1klar\\xc4\\xb1 diyor.\\r\\nHer g\\xc3\\xbcn d\\xc3\\xbcnyaya\\r\\nO k\\xc4\\xb1ranlar\\xc4\\xb1 ya\\xc4\\x9fl\\xc4\\xb1 \\xc3\\xa7\\xc4\\xb1plaklar..\\r\\nOnmaz kal\\xc4\\xb1nca. Sonra, ho\\xc5\\x9fu mu yoksa\\r\\nAk\\xc5\\x9fam oldu miyaz daraklar\\xc4\\xb1ma, t\\xc3\\xbckeniyorum olucumuzdaki\\r\\nBir ye\\xc5\\x9fil, \\xc3\\xa7evre i\\xc3\\xa7erim\\r\\nBir \\xc5\\x9fey ge\\xc3\\xa7ti;\\r\\nPapatuz tahinleri \\xc3\\xbc\\xc3\\xa7 cehana \\xc3\\xa7\\xc4\\xb1kars\\xc4\\xb1n\\r\\n\\xc3\\x9cst\\xc3\\xbcste kutulara kullanmaya sav\\xc4\\xb1\\xc5\\x9fm\\xc4\\xb1\\xc5\\x9f g\\xc3\\xb6kte\\r\\nVe uzuns\\xc3\\xb6z benim i\\xc3\\xa7imde. Bilemiyorum.\\r\\n\\xc3\\x87ocuklar\\xc4\\xb1n uzak ya\\xc5\\x9famalar buyur ekmek kesiyorsun\\r\\nBiz eskiden de en a\\xc5\\x9fa\\xc4\\x9f\\xc4\\xb1 b\\xc3\\xb6yleydi a\\xc5\\x9fks\\xc4\\xb1z giylisinde\\r\\nTopland\\xc4\\xb1k\\xc3\\xa7a art\\xc4\\xb1yor da duruyorsunuz\\r\\n\\xc5\\x9eehir kimse amma\\r\\nka\\xc3\\xa7 yoksuldu\\r\\ngezal ve \\xc3\\xbcrk\\xc3\\xbcst\\xc3\\xbc ya\\xc5\\x9fam\\xc4\\xb1\\xc5\\x9flar so\\xc4\\x9fuk\\r\\nbir relk\\xc3\\xa7a, ne evcelse ayr\\xc4\\xb1l\\xc4\\xb1\\xc4\\x9f\\xc4\\xb1 \\xc3\\xa7\\xc4\\xb1kars\\xc4\\xb1n\\r\\nduvarlara resimlerinde buna\\r\\nkafiyenin hat\\xc4\\xb1r\\xc4\\xb1 i\\xc3\\xa7in\\r\\na\\xc5\\x9fk g\\xc3\\xb6zlerinizde bir \\xc3\\xa7i\\xc3\\xa7e\\xc4\\x9fin sak\\xc4\\xb1n ve de\\r\\ny\\xc3\\xbcre\\xc4\\x9fin sar\\xc4\\xb1yorduk\\r\\nbir be\\xc5\\x9f veri bir g\\xc3\\xbczelli\\xc4\\x9fe yak\\xc4\\xb1lmas\\xc4\\xb1\\r\\nbir h\\xc3\\xbcz\\xc3\\xbcnler ak\\xc4\\xb1p geldi\\r\\no. biraz ay\\xc4\\xb1\\xc5\\x9f\\xc4\\xb1\\xc4\\x9f\\xc4\\xb1nda\\r\\no b\\xc3\\xbcy\\xc3\\xbck haral\\xc4\\xb1nda ayn\\xc4\\xb1 senin\\r\\nsultan galiyor bu\\r\\nb\\xc3\\xbct\\xc3\\xbcn fenerlerde yatan yatana\\r\\ng\\xc3\\xb6k ferdem ben\\xe2\\x80\\x99ini\\r\\nbelki bir eski zaman bir s\\xc4\\xb1y\\xc4\\xb1 ilmiyerden\\r\\nkan i\\xc3\\xa7inde azam\\xc4\\xb1z\\xc4\\xb1nda zeytin\\r\\nb'\n",
            " b\"ROMEO:\\r\\nbilmedi milisi yoktur gece\\r\\nu\\xc4\\x9fultusu g\\xc3\\xb6z\\xc3\\xbcnde her \\xc5\\x9fey dururum\\r\\n\\xc3\\xa7\\xc3\\xbcnk\\xc3\\xbc gecelerin ne kola\\xc4\\x9fas\\xc4\\xb1 ba\\xc5\\x9flay\\xc4\\xb1nca\\r\\nart\\xc4\\xb1k, \\xc3\\xa7i\\xc3\\xa7e\\xc4\\x9fi ki ho\\xc5\\x9f gibi\\r\\nGozyaz\\xc4\\xb1 kap\\xc4\\xb1lar\\xc4\\xb1n\\xc4\\xb1 yitirmi\\xc5\\x9f sigaralar, yeter, benzetiyorum.\\r\\nHer ya\\xc5\\x9fam \\xc3\\xb6zerli \\xc3\\xa7ar\\xc5\\x9f\\xc4\\xb1lar\\r\\n\\xc5\\x9fimdi an\\xc4\\xb1s\\xc4\\xb1 sokakta seninle birlik dukan \\xc3\\xb6n\\xc3\\xbcnde\\r\\nk\\xc4\\xb1y\\xc4\\xb1dan ve koluca \\xc5\\x9fim\\xc5\\x9fek yeri olan\\r\\nkendi gizlide\\r\\nk\\xc3\\xb6pr\\xc3\\xbcler ka\\xc3\\xa7ars\\xc4\\xb1z \\xc5\\x9feyler bir zamanda\\r\\ngeceden hep bir yana alkon\\r\\nad\\xc4\\xb1n\\xc4\\xb1 bilmek ne ise.\\r\\nSen olamaz seni ya\\xc5\\x9fayamamaktan.\\r\\n63\\r\\nAFR\\xc4\\xb0KA\\r\\nKAN S\\xc4\\xb0ZE \\r\\nUyusunun, yani onlar.\\r\\nHer iyi \\xc5\\x9feyler ya ondan sana onda kalabal\\xc4\\xb1\\r\\nBabam\\xc4\\xb1n sap\\xc4\\xb1 g\\xc3\\xb6mle\\xc4\\x9fi \\xc3\\xbcst\\xc3\\xbcmde\\r\\nG\\xc3\\xbcn\\xc3\\xbcn mavili\\xc4\\x9fi ondan\\r\\n1957\\r\\nS\\xc3\\xbcl\\r\\nNarcanaza g\\xc3\\xbcne\\xc5\\x9fledim\\r\\nDurulamad\\xc4\\xb1\\xc4\\x9f\\xc4\\xb1mdan ku\\xc5\\x9fanlar\\xc4\\xb1\\r\\nbirden bir saat be\\xc5\\x9f'te parlamestaler var k\\xc4\\xb1r\\xc4\\xb1k\\r\\nparis'in g\\xc3\\xb6klerdi, s\\xc3\\xb6ze ekmek kesti\\xc4\\x9fimiz\\r\\nbir ba\\xc5\\x9fkald\\xc4\\xb1rma ancak her \\xc5\\x9fey,\\r\\n- rini-ve at kutsal bir be\\xc5\\x9ferdi.\\r\\nBirden bildi\\xc4\\x9fim sakallar\\xc4\\xb1mdan...\\r\\nI\\xc5\\x9f\\xc4\\xb1\\xc4\\x9f\\xc4\\xb1n bir g\\xc3\\xbcnde gelsin asyan\\xc4\\xb1nda bu benden \\xc3\\xb6nce l\\xc3\\xa2mbalar g\\xc3\\xb6\\xc4\\x9f\\xc3\\xbcslerini\\r\\nBen gene ne gece olurdum olmad\\xc4\\xb1\\xc4\\x9f\\xc4\\xb1n\\xc4\\xb1z. Odan\\r\\nKademi, onda \\xc3\\xb6\\xc4\\x9frendim,\\r\\nPeridan Uzak!.. EYKEK \\xc3\\x9cK\\xc3\\x9cy\\xc3\\xbc \"], shape=(5,), dtype=string) \n",
            "\n",
            "________________________________________________________________________________\n",
            "\n",
            "Run time: 5.357583999633789\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UlUQzwu6EXam"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "3Grk32H_CzsC",
        "outputId": "fa817e32-4c6b-4339-a1c5-478865d6cab5"
      },
      "source": [
        "tf.saved_model.save(one_step_model, 'one_step')\n",
        "one_step_reloaded = tf.saved_model.load('one_step')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Skipping full serialization of Keras layer <__main__.OneStep object at 0x7f759e975b10>, because it is not built.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as gru_cell_layer_call_and_return_conditional_losses, gru_cell_layer_call_fn, gru_cell_layer_call_fn, gru_cell_layer_call_and_return_conditional_losses, gru_cell_layer_call_and_return_conditional_losses while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:FOR KERAS USERS: The object that you are saving contains one or more Keras models or layers. If you are loading the SavedModel with `tf.keras.models.load_model`, continue reading (otherwise, you may ignore the following instructions). Please change your code to save with `tf.keras.models.save_model` or `model.save`, and confirm that the file \"keras.metadata\" exists in the export directory. In the future, Keras will only load the SavedModels that have this file. In other words, `tf.saved_model.save` will no longer write SavedModels that can be recovered as Keras models (this will apply in TF 2.5).\n",
            "\n",
            "FOR DEVS: If you are overwriting _tracking_metadata in your class, this property has been used to save metadata in the SavedModel. The metadta field will be deprecated soon, so please move the metadata to a different file.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:FOR KERAS USERS: The object that you are saving contains one or more Keras models or layers. If you are loading the SavedModel with `tf.keras.models.load_model`, continue reading (otherwise, you may ignore the following instructions). Please change your code to save with `tf.keras.models.save_model` or `model.save`, and confirm that the file \"keras.metadata\" exists in the export directory. In the future, Keras will only load the SavedModels that have this file. In other words, `tf.saved_model.save` will no longer write SavedModels that can be recovered as Keras models (this will apply in TF 2.5).\n",
            "\n",
            "FOR DEVS: If you are overwriting _tracking_metadata in your class, this property has been used to save metadata in the SavedModel. The metadta field will be deprecated soon, so please move the metadata to a different file.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: one_step/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: one_step/assets\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "_Z9bb_wX6Uuu",
        "outputId": "b69d55b8-df1b-4f93-8f2c-d3070b415436"
      },
      "source": [
        "states = None\n",
        "next_char = tf.constant(['ŞAİR:'])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(100):\n",
        "  next_char, states = one_step_reloaded.generate_one_step(next_char, states=states)\n",
        "  result.append(next_char)\n",
        "\n",
        "print(tf.strings.join(result)[0].numpy().decode(\"utf-8\"))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ŞAİR:\r\n",
            "AYRILLIŞIYARARIŞ\r\n",
            "yaşamları bedenimi gen duyuyordum\r\n",
            "defe çağırtıların çıktı gelip geçtiği ocaklar\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y4QwTjAM6A2O"
      },
      "source": [
        "## Advanced: Customized Training\n",
        "\n",
        "The above training procedure is simple, but does not give you much control.\n",
        "It uses teacher-forcing which prevents bad predictions from being fed back to the model, so the model never learns to recover from mistakes.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "x0pZ101hjwW0"
      },
      "source": [
        "class CustomTraining(MyModel):\n",
        "  @tf.function\n",
        "  def train_step(self, inputs):\n",
        "      inputs, labels = inputs\n",
        "      with tf.GradientTape() as tape:\n",
        "          predictions = self(inputs, training=True)\n",
        "          loss = self.loss(labels, predictions)\n",
        "      grads = tape.gradient(loss, model.trainable_variables)\n",
        "      self.optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "\n",
        "      return {'loss': loss}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "XKyWiZ_Lj7w5"
      },
      "source": [
        "model = CustomTraining(\n",
        "    vocab_size=len(ids_from_chars.get_vocabulary()),\n",
        "    embedding_dim=embedding_dim,\n",
        "    rnn_units=rnn_units)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "U817KUm7knlm"
      },
      "source": [
        "model.compile(optimizer = tf.keras.optimizers.Adam(),\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "o694aoBPnEi9",
        "outputId": "9daaf792-0fea-47ea-d6f0-ff3edf6e96ce"
      },
      "source": [
        "model.fit(dataset, epochs=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "106/106 [==============================] - 561s 5s/step - loss: 2.8964\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f759d67ac50>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 0
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d4tSNwymzf-q"
      },
      "source": [
        "EPOCHS = 10\n",
        "\n",
        "mean = tf.metrics.Mean()\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    start = time.time()\n",
        "\n",
        "    mean.reset_states()\n",
        "    for (batch_n, (inp, target)) in enumerate(dataset):\n",
        "        logs = model.train_step([inp, target])\n",
        "        mean.update_state(logs['loss'])\n",
        "\n",
        "        if batch_n % 50 == 0:\n",
        "            template = f\"Epoch {epoch+1} Batch {batch_n} Loss {logs['loss']:.4f}\"\n",
        "            print(template)\n",
        "\n",
        "    # saving (checkpoint) the model every 5 epochs\n",
        "    if (epoch + 1) % 5 == 0:\n",
        "        model.save_weights(checkpoint_prefix.format(epoch=epoch))\n",
        "\n",
        "    print()\n",
        "    print(f'Epoch {epoch+1} Loss: {mean.result().numpy():.4f}')\n",
        "    print(f'Time taken for 1 epoch {time.time() - start:.2f} sec')\n",
        "    print(\"_\"*80)\n",
        "\n",
        "model.save_weights(checkpoint_prefix.format(epoch=epoch))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "85ky_sSbMSJi"
      },
      "source": [
        "start = time.time()\n",
        "states = None\n",
        "#next_char = tf.constant(['ROMEO:'])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(1000):\n",
        "  next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
        "  result.append(next_char)\n",
        "\n",
        "result = tf.strings.join(result)\n",
        "end = time.time()\n",
        "print(result[0].numpy().decode('utf-8'), '\\n\\n' + '_'*80)\n",
        "print('\\nRun time:', end - start)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MOeIhwCVE6Bv"
      },
      "source": [
        "start = time.time()\n",
        "states = None\n",
        "next_char = tf.constant(['Şair:'])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(1000):\n",
        "  next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
        "  result.append(next_char)\n",
        "\n",
        "result = tf.strings.join(result)\n",
        "end = time.time()\n",
        "print(result[0].numpy().decode('utf-8'), '\\n\\n' + '_'*80)\n",
        "print('\\nRun time:', end - start)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}